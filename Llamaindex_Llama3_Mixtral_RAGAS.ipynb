{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "I endeavored to construct a chatbot as an **AI-based conversational tool** to enhance the user experience while conducting a research project for ISTAT about the Italian population in 2023. As part of the **Retrieval Augmented Generation (RAG)** method, **two open-source Large Language Models (LLMs)—Llama3 from Meta and Mixtral from Mistra**l—were implemented to generate human-like responses in the information retrieval process and comprehend user queries. Furthermore, the **LlamIndex orchestration framework** was employed to execute this procedure."
      ],
      "metadata": {
        "id": "C6XRKvG6e6yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries and frameworks for the project\n",
        "! pip install llama-index-llms-groq\n",
        "! pip install llama_index\n",
        "! pip install llama_index.embeddings.huggingface\n",
        "! pip install llama_index.vector_stores.chroma\n",
        "! pip install chromadb\n",
        "! pip install datasets\n",
        "! pip install ragas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JoiH7ZJM8sDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules and classes\n",
        "\n",
        "import pandas as pd #Used for handling and manipulating tabular data (e.g., CSV files for dataset loading and preprocessing).\n",
        "import chromadb #A library for working with Chroma, a vector database designed for managing embeddings and performing similarity search.\n",
        "from pathlib import Path #Provides an object-oriented interface for handling file system paths.\n",
        "from llama_index.core import Document #Represents a text document to be indexed. These documents are later processed into embeddings for similarity search.\n",
        "from llama_index.core import VectorStoreIndex #A class for creating and managing vector indices, which are used to efficiently retrieve relevant documents.\n",
        "from llama_index.core import ServiceContext #Provides a centralized way to configure and manage services like LLMs and embedding models.\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt #Allows for the creation of custom input prompts for guiding the behavior of LLMs.\n",
        "from llama_index.core import ServiceContext #Provides a centralized way to configure and manage services like LLMs and embedding models.\n",
        "from llama_index.core.storage.storage_context import StorageContext #Manages the storage and retrieval of index data, embeddings, and other related artifacts.\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding #Embedding generator using HuggingFace models, useful for encoding documents into vector representations.\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore #A vector store backend that uses Chroma for embedding storage and similarity searches.\n",
        "from llama_index.llms.groq import Groq #Integrates Groq-based LLMs for high-performance inference and language generation.\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from datasets import Dataset\n",
        "\n",
        "import os #Sets the API key required for accessing OpenAI services.\n",
        "from getpass import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" # A personal API key must be added.\n",
        "\n",
        "from ragas.testset.generator import TestsetGenerator #Generates test sets for evaluating retrieval-augmented generation systems.\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context #Provides tools for evolving test sets with additional complexity (e.g., multi-context, reasoning).\n",
        "from ragas.integrations.llama_index import evaluate #Evaluates RAG systems built with LlamaIndex using metrics.\n"
      ],
      "metadata": {
        "id": "HutpiHgpgLBE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset prepration**\n",
        "\n",
        "The following code processes a CSV file containing population data for locations in Italy, converting it into a structured collection of Document objects for use in a Retrieval-Augmented Generation (RAG) pipeline. It first reads the dataset using pandas and iterates through each row to generate descriptive text summaries for each location, including details like the type of place, its region and province, and population statistics (male, female, and total). The generated text is split into smaller sections based on a ##### separator, with empty sections removed. Each section is then transformed into a Document object, which serves as a retrievable unit for embedding and querying in a RAG system."
      ],
      "metadata": {
        "id": "-BpptTCwwuo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/popolazione_Italia_2023_Places_updated.csv')\n",
        "\n",
        "text = \"\"\n",
        "for ind in df.index:\n",
        "\n",
        "    text += f\"{df['Luogo'][ind]} is kind of {df['Type of place'][ind]} of the part {df['group_of_region'][ind]} of Italy in province {df['province'][ind]} and region of {df['region'][ind]} that has {df['Maschi'][ind]} male population and {df['Femmine'][ind]} female population and {df['Totale'][ind]} persons as total population#####\"\n",
        "# Step 3: Split the text into individual sections based on the '#####' separator\n",
        "sections = text.split('#####')\n",
        "# Remove any empty sections\n",
        "sections = [section.strip() for section in sections if section.strip()]\n",
        "\n",
        "# Step 4: Convert each section into a Document object\n",
        "documents = [Document(text=section) for section in sections]"
      ],
      "metadata": {
        "id": "LI8aqNcojV8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The following cell sets up a persistent Chroma vector store for efficient document retrieval. A Chroma database (chroma_db_data) is initialized, and a collection named revieww is created to store embeddings. The ChromaVectorStore integrates this collection, and the StorageContext links it with the indexing process. Finally, a VectorStoreIndex is built from the provided documents, enabling optimized retrieval and storage of vectorized document data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gy2II1YLxvHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = chromadb.PersistentClient(path=\"./chroma_db_data\")\n",
        "chroma_collection = client.create_collection(name=\"revieww\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(documents, transformations=transformations, storage_context=storage_context)"
      ],
      "metadata": {
        "id": "pr1EN5sViQm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell configures the settings for the LlamaIndex pipeline, specifying how documents are processed and models are used. The SentenceSplitter is defined to split text into chunks of 250 characters with a 20-character overlap, separated by [#####]. The Groq LLM (Mixtral-8x7b-32768) is set as the default language model, with an optional second LLM (Llama3-8b-8192) provided as a commented-out alternative. The embedding model is set to HuggingFaceEmbedding, leveraging the sentence-transformers/all-MiniLM-L6-v2 for creating vector representations. These settings ensure consistency in text processing and model behavior throughout the pipeline.\n",
        "\n",
        "LLM deployment: I used Groq for employing LLMs because it offers high-performance inference optimized for large models, ensuring faster processing and scalability. Its efficient API integration allows seamless deployment of LLMs like LLama3 and Mixtral."
      ],
      "metadata": {
        "id": "kJTfj2_j4j4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "Settings.text_splitter = SentenceSplitter(chunk_size=250, chunk_overlap=20, separator=\"[#####]\")\n",
        "Settings.llm = Groq( model='Mixtral-8x7b-32768', api_key='') # the personal Groq API key must be added\n",
        "# Settings.llm = Groq( model='Llama3-8b-8192', api_key='') # the second LLMs\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "transformations = [SentenceSplitter(chunk_size=250 ,  chunk_overlap=20, separator=\"[#####]\")]"
      ],
      "metadata": {
        "id": "kGEpVM82zifz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This cell creates a VectorStoreIndex from the provided documents, enabling efficient retrieval in the RAG pipeline. It applies the specified transformations to preprocess the documents before embedding and stores the resulting vectorized data in the storage_context, which links to the configured Chroma vector store. This setup allows the system to efficiently search and retrieve relevant documents based on similarity to a query."
      ],
      "metadata": {
        "id": "h5A3NPOn7Z46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, transformations=transformations, storage_context=storage_context)"
      ],
      "metadata": {
        "id": "31jvrW2BkFEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell converts the VectorStoreIndex into a query engine using index.as_query_engine(). It queries the indexed data for information about Bari's total population, retrieving relevant document chunks. Finally, the result is converted to a string and printed, providing an answer based on the indexed content.**bold text**"
      ],
      "metadata": {
        "id": "z6K445lR7wwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response=query_engine.query(\"Tell me about the total population of Bari?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "JeSfTnKDyRzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An exmaple:"
      ],
      "metadata": {
        "id": "iHT3OVP273-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"Tell me about the total population of Bari?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdhKFWCTkZRx",
        "outputId": "4eec2aa9-0762-4021-8210-2f7b8c23e768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total population of the province of Bari is 1,225,048.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAGAS assessment:\n",
        "\n",
        "RAGAS (Retrieval Augmented Generation Assessment) evaluates the functionality of the RAG system in various capacities. It evaluates the retrieval system's ability to identify pertinent passages, the LLM's ability to utilize them correctly, and the overall quality of the produced output.The efficacy of different components inside the RAG pipeline profoundly influences the overall experience. Ragas provides metrics designed to assess each element of the RAG pipeline independently.\n",
        "\n",
        "\n",
        "**Metrics definition:**\n",
        "\n",
        "These metrics are categorized into two groups: Generation assessment and retrieval\n",
        "assessment, which are described as follows.\n",
        "\n",
        "\n",
        "1.Generation metrics:\n",
        "\n",
        "*   Faithfulness: This metric measures the factual consistency of the generated answer against the given context.\n",
        "*   Answer Correctness: This metric measures the accuracy of the generated answer compared to the ground truth.\n",
        "*   Answer Relevancy: These metrics gauge the relevancy of the retrieved context, calculated based on both the question and contexts.\n",
        "\n",
        "2.Retrieval metrics:\n",
        "\n",
        "\n",
        "\n",
        "*   Context Recall: This metric measures how much the retrieved context aligns with the annotated answer, treated as the ground truth.\n",
        "*   Context Precision: This metric examines how well important information is prioritized in various situations.\n"
      ],
      "metadata": {
        "id": "OMerszMJ77_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from ragas.integrations.llama_index import evaluate\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall, context_entity_recall, answer_similarity, answer_correctness"
      ],
      "metadata": {
        "id": "SCCfVZJggc3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of RAGAS (Retrieval-Augmented Generation for Answering System), ground truth refers to the reference or correct answer against which the system's performance is evaluated. It serves as the benchmark for assessing the accuracy and relevance of the generated responses. In RAGAS assessment for some metrics, the model's output is compared to the ground truth to measure how well it retrieves the correct information from external sources and uses that information to generate an appropriate answer."
      ],
      "metadata": {
        "id": "gmbnqNuY9_Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_questions = [\n",
        "     \"What is the total population of province Prato?\",\n",
        "     \"where is Assisi and what is the male population of there?\",\n",
        "     \"which city in Napoli province is the most populous?\",\n",
        "     \"Compare the population of men and women in the city of Roma.\",\n",
        "     \"Tell me about the difference in sex between the people who live in Cusano Milanino?\",\n",
        "     \"What is the total population of Leini?\",\n",
        "     \"What is the total and Male population of Novara province?\",\n",
        "     \"What is the female population of Palmi?\",\n",
        "     \"What is the percentage of the total population of Italy that resides in the region Lombardy\",\n",
        "     \"What is the ratio of male to female population in the province Latina?\",\n",
        "     \"In Sicilia region, how does the female population compare to the male population in terms of percentage\",\n",
        "     \"tell me about the population of women in Belmonte Mezzagno?\",\n",
        "     \"what is the exact female population of Tivoli?\",\n",
        "     \"how many male populations do reside in Ercolano?\",\n",
        "     \"Tell me about the total population of Bari?\",\n",
        "     \"How many people live in the city of Castellaneta?\",\n",
        "     \"Which region in the Nord-est group has the most evenly balanced gender ratio?\",\n",
        "     \"What is the male population of the region Piemonte?\",\n",
        "     \"what is population of the region Emilia-Romagna?\",\n",
        "     \"How does the male population of Alseno city compare to the female population\"\n",
        "]\n",
        "\n",
        "eval_answers = [\n",
        "     \"The total population in Prato is 259244\",\n",
        "     \"Assisi as a city in province Perugia and region of Umbria in center of Italy has 13339 male population\",\n",
        "     \"The city of Napoli with total population of 917510 is the most populous city in the province of Napoli\",\n",
        "     \"According to the provided data, the population of men in the city of Roma is 1308818, and the population of women is 1446491.\",\n",
        "     \"The male population of Cusano Milanino is 8991, while the female population is 9900. Thus, the difference between the male and female population is 909.\",\n",
        "     \"The total population of Leini is 16294.\",\n",
        "     \"the total population Novara province is 362502 and male population in Novara province is 176980\",\n",
        "     \"The female population of Palmi is 8733\",\n",
        "     \"About 17% of all the people who live in Italy live in the area of Lombardy.\",\n",
        "     \"The male to female population ratio in province Latina is 98%\",\n",
        "     \"In Sicilia, the female population is approximately 51.3%, while the male population is 48.7%\",\n",
        "     \"in the city of Belmonte Mezzagno, the women population is 5530\",\n",
        "     \"The female population of Tivoli is 28032\",\n",
        "     \"the male population in Ercolano is 24407\",\n",
        "     \"Bari has a total population of 1225048 as a province, and 316736 as a city.\",\n",
        "     \"the total population of Castellaneta is 16220 people\",\n",
        "     \"The most balanced gender ratio in the Nord-est group is found in Veneto\",\n",
        "     \"The male population of the region Piemonte is 2,072,771\",\n",
        "     \"The total population of the region Emilia-Romagna is 4435758\",\n",
        "     \"the male population in Alseno city is 2315, and the female population is 2374\"\n",
        "]\n",
        "\n",
        "examples = [\n",
        "    {\"query\": q, \"ground_truth\": [eval_answers[i]]}\n",
        "    for i, q in enumerate(eval_questions)\n",
        "]"
      ],
      "metadata": {
        "id": "yrS_IeGuvAVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using GPT 3.5 for LLM as ajudge\n",
        "evaluator_llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "3_iVm3RGCfG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_queries(queries):\n",
        "    results = []\n",
        "    for query in eval_questions:\n",
        "        # Call query_engine.query() for each query\n",
        "        response = query_engine.query(query)\n",
        "        results.append(response)\n",
        "    return results\n",
        "query_results = process_queries(eval_questions)\n",
        "query_results"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1gGnKg6rFnBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "contexts = []\n",
        "for question in eval_questions:\n",
        "  result = query_engine.query(question)\n",
        "  results.append(result.response)\n",
        "  context_list = [source_node.text for source_node in result.source_nodes]\n",
        "  contexts.append(context_list)"
      ],
      "metadata": {
        "id": "A9zQmXVtD-TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {\n",
        "    \"question\": eval_questions,\n",
        "    \"answer\": results,\n",
        "    \"contexts\": contexts,\n",
        "    \"ground_truth\": eval_answers\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(d)\n",
        "\n",
        "\n",
        "# Ensure the query_engine returns a valid response object\n",
        "score = evaluate(\n",
        "    query_engine=query_engine,\n",
        "    dataset=dataset,\n",
        "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall, answer_correctness],\n",
        ")\n",
        "\n",
        "# Convert to pandas DataFrame and save to CSV\n",
        "score_df = score.to_pandas()\n",
        "score_df.to_csv(\"EvaluationScores.csv\", encoding=\"utf-8\", index=False)\n",
        "\n",
        "# Print the types of objects returned by the query engine for debugging\n",
        "print([type(r) for r in query_results])"
      ],
      "metadata": {
        "id": "nfBfSnx4iDql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_df[['faithfulness','answer_relevancy', 'context_precision', 'context_recall','answer_correctness']].mean(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "wAF29Z5nQbYX",
        "outputId": "43ece96a-95a0-4fed-dac7-b2b8185c9bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "faithfulness          0.908333\n",
              "answer_relevancy      0.822824\n",
              "context_precision     0.850000\n",
              "context_recall        0.675000\n",
              "answer_correctness    0.659971\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>faithfulness</th>\n",
              "      <td>0.908333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer_relevancy</th>\n",
              "      <td>0.822824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>context_precision</th>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>context_recall</th>\n",
              "      <td>0.675000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer_correctness</th>\n",
              "      <td>0.659971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    }
  ]
}
